<html>
   <head>
      <meta content="text/html; charset=UTF-8" http-equiv="content-type">
      <style type="text/css">ol.lst-kix_gh1tqxxge7r0-0.start{counter-reset:lst-ctn-kix_gh1tqxxge7r0-0 0}.lst-kix_gh1tqxxge7r0-8>li{counter-increment:lst-ctn-kix_gh1tqxxge7r0-8}ol.lst-kix_gh1tqxxge7r0-5.start{counter-reset:lst-ctn-kix_gh1tqxxge7r0-5 0}.lst-kix_gh1tqxxge7r0-4>li{counter-increment:lst-ctn-kix_gh1tqxxge7r0-4}ol.lst-kix_gh1tqxxge7r0-6{list-style-type:none}ol.lst-kix_gh1tqxxge7r0-5{list-style-type:none}.lst-kix_gh1tqxxge7r0-7>li{counter-increment:lst-ctn-kix_gh1tqxxge7r0-7}ol.lst-kix_gh1tqxxge7r0-8{list-style-type:none}.lst-kix_gh1tqxxge7r0-1>li{counter-increment:lst-ctn-kix_gh1tqxxge7r0-1}.lst-kix_gh1tqxxge7r0-5>li:before{content:"" counter(lst-ctn-kix_gh1tqxxge7r0-5,lower-roman) ". "}ol.lst-kix_gh1tqxxge7r0-7{list-style-type:none}.lst-kix_gh1tqxxge7r0-4>li:before{content:"" counter(lst-ctn-kix_gh1tqxxge7r0-4,lower-latin) ". "}ol.lst-kix_gh1tqxxge7r0-8.start{counter-reset:lst-ctn-kix_gh1tqxxge7r0-8 0}ol.lst-kix_gh1tqxxge7r0-0{list-style-type:none}.lst-kix_gh1tqxxge7r0-1>li:before{content:"" counter(lst-ctn-kix_gh1tqxxge7r0-1,lower-latin) ". "}.lst-kix_gh1tqxxge7r0-3>li:before{content:"" counter(lst-ctn-kix_gh1tqxxge7r0-3,decimal) ". "}ol.lst-kix_gh1tqxxge7r0-2{list-style-type:none}ol.lst-kix_gh1tqxxge7r0-1{list-style-type:none}ol.lst-kix_gh1tqxxge7r0-3.start{counter-reset:lst-ctn-kix_gh1tqxxge7r0-3 0}ol.lst-kix_gh1tqxxge7r0-4{list-style-type:none}.lst-kix_gh1tqxxge7r0-2>li:before{content:"" counter(lst-ctn-kix_gh1tqxxge7r0-2,lower-roman) ". "}ol.lst-kix_gh1tqxxge7r0-3{list-style-type:none}ol.lst-kix_gh1tqxxge7r0-1.start{counter-reset:lst-ctn-kix_gh1tqxxge7r0-1 0}ol.lst-kix_gh1tqxxge7r0-6.start{counter-reset:lst-ctn-kix_gh1tqxxge7r0-6 0}.lst-kix_gh1tqxxge7r0-0>li:before{content:"" counter(lst-ctn-kix_gh1tqxxge7r0-0,decimal) ". "}.lst-kix_gh1tqxxge7r0-0>li{counter-increment:lst-ctn-kix_gh1tqxxge7r0-0}.lst-kix_gh1tqxxge7r0-3>li{counter-increment:lst-ctn-kix_gh1tqxxge7r0-3}.lst-kix_gh1tqxxge7r0-6>li{counter-increment:lst-ctn-kix_gh1tqxxge7r0-6}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_gh1tqxxge7r0-6>li:before{content:"" counter(lst-ctn-kix_gh1tqxxge7r0-6,decimal) ". "}.lst-kix_gh1tqxxge7r0-7>li:before{content:"" counter(lst-ctn-kix_gh1tqxxge7r0-7,lower-latin) ". "}ol.lst-kix_gh1tqxxge7r0-4.start{counter-reset:lst-ctn-kix_gh1tqxxge7r0-4 0}.lst-kix_gh1tqxxge7r0-8>li:before{content:"" counter(lst-ctn-kix_gh1tqxxge7r0-8,lower-roman) ". "}ol.lst-kix_gh1tqxxge7r0-2.start{counter-reset:lst-ctn-kix_gh1tqxxge7r0-2 0}.lst-kix_gh1tqxxge7r0-5>li{counter-increment:lst-ctn-kix_gh1tqxxge7r0-5}ol.lst-kix_gh1tqxxge7r0-7.start{counter-reset:lst-ctn-kix_gh1tqxxge7r0-7 0}.lst-kix_gh1tqxxge7r0-2>li{counter-increment:lst-ctn-kix_gh1tqxxge7r0-2}ol{margin:0;padding:0}table td,table th{padding:0}.c4{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:12pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c5{padding-top:12pt;padding-bottom:12pt;line-height:1.5;orphans:2;widows:2;text-align:left;height:11pt}.c9{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c7{padding-top:12pt;padding-bottom:12pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c2{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c12{color:#000000;text-decoration:none;vertical-align:baseline}.c13{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c11{text-decoration:none;vertical-align:baseline;font-style:normal}.c15{font-weight:400;font-size:11pt;font-family:"Times New Roman"}.c1{font-size:20pt;font-weight:700;font-family:"Times New Roman"}.c0{font-size:12pt;font-weight:700;font-family:"Times New Roman"}.c8{font-weight:400;font-size:12pt;font-family:"Times New Roman"}.c14{padding:0;margin:0}.c10{color:#0000ff}.c6{font-style:italic}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style>
   </head>
   <body class="c13 doc-content">
      <p class="c3"><span class="c1">Driver Assistance System</span><span class="c2 c15">&nbsp;</span></p>
      <p class="c3"><span class="c0 c10">Thiong Abraham Manar</span><span class="c8 c10 c11">&nbsp;</span></p>
      <p class="c3"><span class="c2 c8">University of Juba </span></p>
      <p class="c7"><span class="c2 c0">&nbsp;</span></p>
      <p class="c7"><span class="c2 c0">ABSTRACT</span></p>
      <p class="c7"><span class="c2 c8">In Juba, drivers frequently disregard posted speed limits, a widespread issue that significantly elevates the risk of road accidents. Detecting these traffic speed signs using AI offers a crucial opportunity to enhance road safety by providing real-time data or alerts that could deter speeding. The DAS project tries to solve this problem by examining the application of computer vision in detecting speed limit signs using YOLOv11pt, an advanced object detection model. Initially targeting static image recognition then advancing to real time recognition i.e. real videos, the system is designed to accurately locate speed limit signs before being deployed for real-time video monitoring. Leveraging YOLOv11pt&#39;s improved speed and precision over previous versions, the system processes input frames to detect signs under varying lighting, angles, and occlusions.</span></p>
      <p class="c7"><span class="c2 c8">The methodology involves dataset collection, annotation using label studio, and model training using blended sets of speed limit signs datasets. Post-training, the model is evaluated on static images to assess detection accuracy, followed by integration into a real-time pipeline using OpenCV for video processing. Performance metrics such as precision, recall, and F1-score are analysed, alongside inference speed (FPS) to ensure real-time applicability.</span></p>
      <p class="c7"><span class="c2 c8">Preliminary results demonstrate robust detection in static images with 83% mAP50-95, while real-time video tests show promising frame rates suitable for deployment in traffic monitoring systems. Future enhancements include optimizing the model for edge devices and expanding detection to other traffic signs.</span></p>
      <p class="c7"><span class="c2 c8">&nbsp;Keywords: Computer Vision, YOLOv11pt, Speed Limit Detection, Real-Time Object Detection, Traffic Sign Recognition.</span></p>
      <p class="c5"><span class="c2 c8"></span></p>
      <p class="c7"><span class="c2 c0">METHODOLOGY</span></p>
      <p class="c7"><span class="c2 c8">In this project, we implement an intelligent two-stage computer vision pipeline to automatically detect and interpret speed limit signs from real-time camera footage. The first stage involves utilizing YOLOv8pt, a state-of-the-art object detection model, to accurately localize and identify speed limit signs within the camera feed. Once these signs are detected, the second stage employs a Convolutional Neural Network (CNN) specifically designed to recognize and extract the numeric speed value displayed on the sign.</span></p>
      <p class="c7"><span class="c2 c8">The system is designed for real-world applications. By comparing the detected speed limit with the vehicle&#39;s current speed, the model can determine whether the driver is exceeding the posted limit. If an over speeding scenario is detected, the system can trigger an audible or visual alert, notifying the driver to adjust their speed accordingly.</span></p>
      <p class="c7"><span class="c2 c8">To ensure robustness, the YOLOv8pt model is fine-tuned on a diverse dataset of traffic signs under varying lighting and weather conditions, improving detection accuracy. Meanwhile, the CNN model is trained on a large - annotated dataset of speed limit numbers to minimize misclassification. Future enhancements could include extending the system to recognize other traffic signs for comprehensive road safety.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 651.00px; height: 170.93px;"><img alt="" src="image1.png" style="width: 651.00px; height: 170.93px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>
      <p class="c7"><span class="c2 c0">&nbsp;</span></p>
      <p class="c7"><span class="c2 c0">&nbsp;</span></p>
      <p class="c5"><span class="c2 c0"></span></p>
      <p class="c5"><span class="c2 c0"></span></p>
      <p class="c7"><span class="c2 c0">REFERENCES</span></p>
      <ol class="c14 lst-kix_gh1tqxxge7r0-0 start" start="1">
         <li class="c4 li-bullet-0"><span class="c0">Redmon, J., &amp; Farhadi, A. (2018). </span><span class="c0 c6">YOLOv3: An Incremental Improvement</span><span class="c2 c0">. arXiv preprint arXiv:1804.02767.</span></li>
         <li class="c4 li-bullet-0"><span class="c0">Bochkovskiy, A., Wang, C. Y., &amp; Liao, H. Y. M. (2020). </span><span class="c0 c6">YOLOv4: Optimal Speed and Accuracy of Object Detection</span><span class="c2 c0">. arXiv preprint arXiv:2004.10934.</span></li>
         <li class="c4 li-bullet-0"><span class="c0">Liu, W., et al. (2016). </span><span class="c0 c6">SSD: Single Shot MultiBox Detector</span><span class="c0">. In </span><span class="c0 c6">European Conference on Computer Vision (ECCV)</span><span class="c0 c2">.</span></li>
         <li class="c4 li-bullet-0"><span class="c0">Stallkamp, J., et al. (2012). </span><span class="c0 c6">The German Traffic Sign Recognition Benchmark: A Multi-class Classification Competition</span><span class="c0">. In </span><span class="c0 c6">IEEE International Joint Conference on Neural Networks (IJCNN)</span><span class="c2 c0">.</span></li>
         <li class="c4 li-bullet-0"><span class="c0">Girshick, R. (2015). </span><span class="c0 c6">Fast R-CNN</span><span class="c0">. In </span><span class="c0 c6">IEEE International Conference on Computer Vision (ICCV)</span><span class="c2 c0">.</span></li>
         <li class="c4 li-bullet-0"><span class="c2 c0">Howard, A., et al. (2017)</span></li>
         <li class="c4 li-bullet-0"><span class="c0">Ren, S., et al. (2015). </span><span class="c0 c6">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</span><span class="c2 c0">.</span></li>
         <li class="c4 li-bullet-0"><span class="c0">Everingham, M., et al. (2010). </span><span class="c12 c0 c6">The Pascal Visual Object Classes (VOC)</span></li>
         <li class="c4 li-bullet-0"><span class="c0">Geiger, A., et al. (2013). </span><span class="c0 c6">Vision meets Robotics: The KITTI Dataset</span><span class="c0">. </span><span class="c0 c6 c12">International</span></li>
         <li class="c4 li-bullet-0"><span class="c0">He, K., et al. (2016). </span><span class="c0 c6">Deep Residual Learning for Image Recognition</span><span class="c0">. In </span><span class="c12 c0 c6">IEEE </span></li>
      </ol>
      <p class="c9"><span class="c2 c0"></span></p>
   </body>
</html>
